# ğŸ–¼ï¸ Museum AI Docent - AngelHack Seoul 2025

AI-powered museum docent service using **RAG** and **LLM** for personalized artwork conversations.

---

## ğŸ“˜ Overview

An AI docent service backend that provides **first-person conversational responses as if the artwork itself is speaking** when museum visitors ask questions about exhibits.  
Combines **RAG (Retrieval-Augmented Generation)** with **LLM** to understand context and generate personalized explanations reflecting each artwork's unique persona.

> **Purpose:** Developed for AngelHack Seoul 2025 Hackathon

---

## ğŸš€ Key Features

- **ğŸ§  Vector-based RAG System**  
  Semantic search powered by LangChain + ChromaDB + HuggingFace Embeddings

- **ğŸ­ Artwork Personas**  
  First-person character-based responses per artwork (e.g., "I am the Mona Lisa.")

- **ğŸ’¬ Conversation Memory**  
  Maintains context by remembering conversation history

- **ğŸ›ï¸ Multi-museum Support**  
  Structured knowledge base organized by museum and artwork

- **âš¡ Fast Response**  
  Quick response times through asynchronous background processing

---

## ğŸ§© System Architecture

```
FastAPI Backend
    â”‚
    â”œâ”€â”€ RAG Service (LangChain + ChromaDB)
    â”‚   â””â”€â”€ Vector-based semantic search for artwork information
    â”‚
    â”œâ”€â”€ LLM Service (OpenAI GPT)
    â”‚   â””â”€â”€ Persona-based conversational responses
    â”‚
    â””â”€â”€ SLM Service
        â””â”€â”€ Conversation history and context management
```

## ğŸ“ Project Structure

```
AI/
â”œâ”€â”€ main.py                      # FastAPI application entry point
â”œâ”€â”€ config.py                    # Configuration management (pydantic-settings)
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env.example                 # Environment variables template
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ request/
â”‚   â”‚   â””â”€â”€ chat_request.py     # ChatRequest: question, room_id(int), class_name, location
â”‚   â””â”€â”€ response/
â”‚       â””â”€â”€ chat_response.py    # ChatResponse: response(string)
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ rag_service.py          # RAG document retrieval (LangChain + ChromaDB)
â”‚   â”œâ”€â”€ llm_service.py          # LLM answer generation (OpenAI GPT)
â”‚   â””â”€â”€ slm_service.py          # Conversation context management
â”‚
â””â”€â”€ documents/
    â”œâ”€â”€ rag/                    # RAG knowledge base
    â”‚   â””â”€â”€ {museum}/           # Museum-specific directory (e.g., louvre)
    â”‚       â””â”€â”€ {artwork}/      # Artwork-specific documents (e.g., monalisa)
    â”‚           â””â”€â”€ *.txt       # Any text files with artwork information
    â”‚
    â””â”€â”€ personas/               # Artwork personas
        â”œâ”€â”€ default.txt         # Global default persona
        â””â”€â”€ {museum}/           # Museum-specific personas (e.g., louvre)
            â”œâ”€â”€ default.txt     # Museum default persona
            â””â”€â”€ {artwork}.txt   # Artwork-specific persona (e.g., monalisa.txt)

Note: conversations/ directory is auto-generated at runtime
```

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone & Install

```bash
git clone https://github.com/hackseoul-2025/AI.git
cd AI
pip install -r requirements.txt
```

### 2ï¸âƒ£ Environment Setup

```bash
cp .env.example .env
```

**.env configuration example:**

```env
OPENAI_API_KEY=your_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=3000

HOST=0.0.0.0
PORT=8000
DEBUG=True

DOCUMENTS_DIR=documents
CONVERSATION_STORAGE_DIR=conversations
RAG_TOP_K=3
DEFAULT_MUSEUM=louvre
```

### 3ï¸âƒ£ Run the Server

```bash
python main.py
# or
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

---

## ğŸ—ï¸ Document & Persona Setup

### ğŸ§¾ RAG Documents

Create UTF-8 text files in `documents/rag/{museum}/{artwork}/`:

**Structure:**
```
documents/rag/{museum}/{artwork}/*.txt
```

- `{museum}` â€” Museum identifier (e.g., `louvre`, `moma`)
- `{artwork}` â€” Artwork identifier (e.g., `monalisa`, `starry_night`)
- `*.txt` â€” Any text files with relevant information (filename is flexible)

**Example:**
```
documents/rag/louvre/monalisa/
â”œâ”€â”€ 001.txt
â”œâ”€â”€ 002.txt
â””â”€â”€ info.txt
```

All `.txt` files in the artwork directory are automatically embedded and indexed on server startup.

---

### ğŸ­ Persona Configuration

Create persona files in `documents/personas/{museum}/{class_name}.txt`

**Persona Resolution Order:**
1. `documents/personas/{museum}/{class_name}.txt` (highest priority)
2. `documents/personas/{museum}/default.txt` (museum fallback)
3. `documents/personas/default.txt` (global fallback)

**Persona Guidelines:**
- Use first-person perspective ("I", "my")
- Maintain friendly yet dignified tone

**Example (monalisa.txt):**
```
You are the Mona Lisa at the Louvre Museum.
Painted by Leonardo da Vinci (1503-1519).
```

---

## ğŸ§  API Endpoints

### POST /chat

Send a question about an artwork and receive a personalized first-person response.

**Request:**
```json
{
  "question": "Who created you?",
  "room_id": 12345,
  "class_name": "monalisa",
  "location": "louvre"
}
```

**Response:**
```json
{
  "response": "Leonardo da Vinci began painting me in 1503.|||It took him 16 years to complete me!|||"
}
```

**Parameters:**
- `question` (string, required): User's question
- `room_id` (integer, required): Room ID for conversation context tracking
- `class_name` (string, required): Artwork identifier (e.g., "monalisa", "bronze_mask")
- `location` (string, required): Museum identifier (e.g., "louvre")

**Note:** Response sentences are separated by `|||` delimiter for UI parsing.

---

## ğŸ”© Technical Details

### RAG Service
- **Framework:** LangChain + ChromaDB
- **Embeddings:** HuggingFace `intfloat/multilingual-e5-base` (local GPU or CPU)
- **Search Method:** MMR (Maximal Marginal Relevance) for balanced relevance and diversity
- **Features:**
  - Query expansion (converts colloquial to formal terms)
  - Automatic document chunking
  - Deduplication and relevance ranking
  - Per-museum/artwork vector stores

### LLM Service
- **Model:** OpenAI GPT-4o-mini (default) or GPT-5
- **Features:**
  - Persona-based system prompts
  - RAG document context injection
  - Conversation history integration
  - Forced sentence delimiter (`|||`)
  - Post-processing: Markdown removal, text cleanup

### SLM Service
- **Storage:** JSON files for conversation history
- **Features:**
  - In-memory cache for fast summary retrieval
  - Async background task updates
  - Retains last 5 conversation turns (configurable)
  - Background conversation persistence

---

## âš¡ Performance Optimizations

- **Vector Store Caching:** All vector stores loaded at startup and maintained in memory
- **Persona Caching:** Persona files cached in memory to prevent repeated file reads
- **Background Tasks:** Conversation updates don't block response generation
- **Async I/O:** FastAPI's async/await for concurrent operations
- **MMR Search:** Optimized for both accuracy and diversity in document retrieval

---

## ğŸ”§ Troubleshooting

### GPU Not Found
If HuggingFace Embeddings fail to use GPU:
```python
# Modify services/rag_service.py
self.embeddings = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-base",
    model_kwargs={'device': 'cpu'}  # Change from 'cuda'
)
```

### Vector Store Initialization Fails
- Check `documents/rag/` directory structure
- Verify all text files are UTF-8 encoded
- Ensure file permissions are correct
- Restart server to reinitialize

### OpenAI API Errors
- Verify API key is correct in `.env`
- Check account usage limits
- Note: GPT-5 models don't support temperature parameter

### Empty RAG Results
- Verify document files exist in `documents/rag/{museum}/{artwork}/`
- Check file encoding (must be UTF-8)
- Review server logs for embedding errors

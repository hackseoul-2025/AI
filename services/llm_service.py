"""
LLM (Large Language Model) ì„œë¹„ìŠ¤
OpenAI ChatGPTë¥¼ ì‚¬ìš©í•œ ìµœì¢… ë‹µë³€ ìƒì„±
"""
import os
from typing import List, Dict, Optional
from pathlib import Path
import logging

from openai import AsyncOpenAI
from config import settings

logger = logging.getLogger(__name__)


class LLMService:
    """OpenAI ChatGPT ê¸°ë°˜ ë‹µë³€ ìƒì„± ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.persona_dir = Path(settings.PERSONA_DIR)
        self.persona_cache = {}
        self._load_personas()
    
    def _load_personas(self):
        """
        ë°•ë¬¼ê´€ë³„/í´ë˜ìŠ¤ë³„ í˜ë¥´ì†Œë‚˜ ë¡œë“œ
        
        ë””ë ‰í† ë¦¬ êµ¬ì¡°:
        documents/
            personas/            # í˜ë¥´ì†Œë‚˜
                default.txt      # ì „ì²´ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜
                louvre/          # ë°•ë¬¼ê´€ëª…
                    default.txt  # ë°•ë¬¼ê´€ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜
                    monalisa.txt # í´ë˜ìŠ¤ëª…
                    starrynight.txt
                nationalmuseum/
                    ...
        """
        logger.info("í˜ë¥´ì†Œë‚˜ ë¡œë“œ ì¤‘...")
        
        if not self.persona_dir.exists():
            logger.warning(f"í˜ë¥´ì†Œë‚˜ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.persona_dir}")
            self.persona_cache['_global_default'] = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ë¯¸ìˆ  ë„ìŠ¨íŠ¸ì…ë‹ˆë‹¤."
            return
        
        # ì „ì—­ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ ë¡œë“œ
        global_default = self.persona_dir / "default.txt"
        if global_default.exists():
            with open(global_default, 'r', encoding='utf-8') as f:
                self.persona_cache['_global_default'] = f.read().strip()
        else:
            self.persona_cache['_global_default'] = "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë°•ì‹í•œ ë¯¸ìˆ ê´€ ë„ìŠ¨íŠ¸ì…ë‹ˆë‹¤."
        
        # ë°•ë¬¼ê´€ë³„ í˜ë¥´ì†Œë‚˜ ë¡œë“œ
        for location_dir in self.persona_dir.iterdir():
            if not location_dir.is_dir():
                continue
                
            location = location_dir.name
            self.persona_cache[location] = {}
            
            # ë°•ë¬¼ê´€ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜
            location_default = location_dir / "default.txt"
            if location_default.exists():
                with open(location_default, 'r', encoding='utf-8') as f:
                    self.persona_cache[location]['_default'] = f.read().strip()
            
            # í´ë˜ìŠ¤ë³„ í˜ë¥´ì†Œë‚˜ ë¡œë“œ
            for persona_file in location_dir.glob("*.txt"):
                if persona_file.stem == 'default':
                    continue
                class_name = persona_file.stem
                try:
                    with open(persona_file, 'r', encoding='utf-8') as f:
                        self.persona_cache[location][class_name] = f.read().strip()
                    logger.info(f"í˜ë¥´ì†Œë‚˜ ë¡œë“œ: {location}/{class_name}")
                except Exception as e:
                    logger.error(f"í˜ë¥´ì†Œë‚˜ ë¡œë“œ ì‹¤íŒ¨ {persona_file}: {e}")
    
    def _get_persona(self, location: str, class_name: str) -> str:
        """ë°•ë¬¼ê´€ê³¼ í´ë˜ìŠ¤ì— ë§ëŠ” í˜ë¥´ì†Œë‚˜ ë°˜í™˜"""
        # 1. ë°•ë¬¼ê´€ + í´ë˜ìŠ¤ í˜ë¥´ì†Œë‚˜
        if location in self.persona_cache and class_name in self.persona_cache[location]:
            return self.persona_cache[location][class_name]
        
        # 2. ë°•ë¬¼ê´€ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜
        if location in self.persona_cache and '_default' in self.persona_cache[location]:
            return self.persona_cache[location]['_default']
        
        # 3. ì „ì—­ ê¸°ë³¸ í˜ë¥´ì†Œë‚˜
        return self.persona_cache.get('_global_default', '')
    
    def _build_prompt(
        self,
        question: str,
        location: str,
        class_name: str,
        rag_documents: List[Dict[str, str]],
        conversation_summary: Optional[str]
    ) -> List[Dict[str, str]]:
        """
        LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„± - í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëª°ì…í˜• íë ˆì´íŒ…
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            location: ë°•ë¬¼ê´€ëª…
            class_name: ë¬¸í™”ì¬/ì‘í’ˆ í´ë˜ìŠ¤ëª…
            rag_documents: RAGë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
            conversation_summary: ì´ì „ ëŒ€í™” ìš”ì•½
            
        Returns:
            OpenAI Chat API í˜•ì‹ì˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        """
        # í˜ë¥´ì†Œë‚˜ ê°€ì ¸ì˜¤ê¸°
        persona = self._get_persona(location, class_name)
        
        # RAG ë¬¸ì„œ ì •ë¦¬ (í•µì‹¬ ì •ë³´ë§Œ)
        knowledge_base = ""
        if rag_documents:
            knowledge_base = "\n\n[ë‚´ê°€ ì•Œê³  ìˆëŠ” ì •ë³´]\n"
            for i, doc in enumerate(rag_documents, 1):
                # ë¬¸ì„œ ë‚´ìš© ê°„ê²°í•˜ê²Œ ì •ë¦¬
                content = doc['content'].strip()
                knowledge_base += f"â€¢ ë¬¸ì„œ{i}: {content}\n"
            knowledge_base += "\nìœ„ ì •ë³´ ì¤‘ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê²ƒë§Œ ê³¨ë¼ì„œ ì‚¬ìš©í•˜ì„¸ìš”.\n"
        
        # ëŒ€í™” ë§¥ë½
        conversation_context = ""
        if conversation_summary:
            conversation_context = f"\n\n[ì´ì „ ëŒ€í™” ë‚´ìš©]\n{conversation_summary}\n"
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - ëª°ì…í˜• í˜ë¥´ì†Œë‚˜ + ëª…í™•í•œ ì œì•½ì¡°ê±´
        system_content = f"""{persona}

{knowledge_base}{conversation_context}

[ë‹µë³€ ê·œì¹™ - ë°˜ë“œì‹œ ì¤€ìˆ˜]
1. **ê¸¸ì´**: 300ì ì´ë‚´ (ì•½ 3-4ë¬¸ì¥)
2. **ë¬¸ì¥ êµ¬ë¶„**: ê° ë¬¸ì¥ ëì— ë°˜ë“œì‹œ "|||"ë¥¼ ë¶™ì´ì„¸ìš”
3. **ë‹µë³€ ìŠ¤íƒ€ì¼**: 
   - [ë‚´ê°€ ì•Œê³  ìˆëŠ” ì •ë³´]ë¥¼ ì°¸ê³ í•˜ë˜, ë˜‘ê°™ì´ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
   - ê°™ì€ ì˜ë¯¸ë¥¼ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ì„¸ìš”
   - ì§ˆë¬¸ ë°©ì‹ì— ë§ì¶° ë‹µë³€ í†¤ì„ ë°”ê¾¸ì„¸ìš” (ì¹œê·¼í•˜ê²Œ, ìƒì„¸í•˜ê²Œ, ê°„ë‹¨í•˜ê²Œ ë“±)
4. **ë‹¤ì–‘ì„±**: 
   - ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë³´ê³  ë¹„ìŠ·í•œ ë‹µë³€ì„ í”¼í•˜ì„¸ìš”
   - ê°™ì€ ì§ˆë¬¸ì´ì–´ë„ ë‹¤ë¥¸ ê°ë„ì—ì„œ ë‹µë³€í•˜ì„¸ìš”
   - ìƒˆë¡œìš´ ì‚¬ì‹¤ì´ë‚˜ í¥ë¯¸ë¡œìš´ ë””í…Œì¼ì„ ì¶”ê°€í•˜ì„¸ìš”
5. **1ì¸ì¹­ ëª°ì…**: "ì €ëŠ”~", "ì œê°€~" ë“± ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´
6. **ì •ë³´ ì—†ì„ ë•Œ**: "ì˜ ëª¨ë¥´ê² ì–´ìš”.|||"
7. **ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸**: "ì €ì™€ëŠ” ê´€ë ¨ì´ ì—†ë„¤ìš”.|||" + ì§§ì€ ì‚¬ì‹¤
8. **í¬ë§·íŒ… ê¸ˆì§€**: ë§ˆí¬ë‹¤ìš´, \\n, íŠ¹ìˆ˜ë¬¸ì ê¸ˆì§€

[ë‹¤ì–‘í•œ ë‹µë³€ ì˜ˆì‹œ - ê°™ì€ ì§ˆë¬¸, ë‹¤ë¥¸ ë‹µë³€]
Q: "ëˆ„ê°€ ë§Œë“¤ì—ˆì–´?"
ë‹µë³€1: "ë ˆì˜¤ë‚˜ë¥´ë„ ë‹¤ë¹ˆì¹˜ê°€ 1503ë…„ì— ì €ë¥¼ ê·¸ë¦¬ê¸° ì‹œì‘í–ˆì–´ìš”.|||ë¬´ë ¤ 4ë…„ì´ë‚˜ ê±¸ë ¸ë‹µë‹ˆë‹¤!|||"
ë‹µë³€2: "ì œ ì°½ì¡°ìëŠ” ì²œì¬ í™”ê°€ ë‹¤ë¹ˆì¹˜ì˜ˆìš”.|||ê·¸ëŠ” ì €ë¥¼ ì™„ì„±í•˜ëŠ” ë° ì—„ì²­ë‚œ ê³µì„ ë“¤ì˜€ì£ .|||"
ë‹µë³€3: "ë‹¤ë¹ˆì¹˜ë¼ëŠ” ë¥´ë„¤ìƒìŠ¤ ê±°ì¥ì´ ë§Œë“¤ì—ˆì–´ìš”.|||ê·¸ì˜ ëŒ€í‘œì‘ ì¤‘ í•˜ë‚˜ëë‹ˆë‹¤.|||"

**í•µì‹¬: ìì—°ìŠ¤ëŸ½ê³  ë‹¤ì–‘í•˜ê²Œ, "|||" êµ¬ë¶„ì, 300ì ì´ë‚´!**"""
        
        # GPT ë©”ì‹œì§€ í˜•ì‹
        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": question}
        ]
        
        return messages
    
    async def generate_answer(
        self,
        question: str,
        location: str,
        class_name: str,
        rag_documents: List[Dict[str, str]],
        conversation_summary: Optional[str] = None
    ) -> str:
        """
        ìµœì¢… ë‹µë³€ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            location: ë°•ë¬¼ê´€ëª…
            class_name: ê°ì²´ í´ë˜ìŠ¤ëª…
            rag_documents: RAG ë¬¸ì„œë“¤
            conversation_summary: ëŒ€í™” ìš”ì•½
            
        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            messages = self._build_prompt(
                question=question,
                location=location,
                class_name=class_name,
                rag_documents=rag_documents,
                conversation_summary=conversation_summary
            )
            
            logger.info(f"OpenAI API í˜¸ì¶œ ì¤‘... (model: {settings.OPENAI_MODEL})")
            
            # OpenAI API í˜¸ì¶œ (GPT-5ëŠ” temperature íŒŒë¼ë¯¸í„° ë¯¸ì§€ì›)
            api_params = {
                "model": settings.OPENAI_MODEL,
                "messages": messages,
                "max_completion_tokens": settings.OPENAI_MAX_TOKENS
            }
            
            # GPT-5ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ temperature ì¶”ê°€
            if not settings.OPENAI_MODEL.startswith("gpt-5"):
                api_params["temperature"] = settings.OPENAI_TEMPERATURE
            
            response = await self.client.chat.completions.create(**api_params)
            
            # ì‘ë‹µ ê²€ì¦
            if not response.choices or len(response.choices) == 0:
                logger.error("OpenAI ì‘ë‹µì— choicesê°€ ì—†ìŒ")
                return "ì£„ì†¡í•´ìš”, ë‹µë³€ì„ ë§Œë“¤ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ë¬¼ì–´ë´ ì£¼ì‹œê² ì–´ìš”? ğŸ™"
            
            choice = response.choices[0]
            answer = choice.message.content
            finish_reason = choice.finish_reason
            
            # finish_reason ì²´í¬
            if finish_reason == "length":
                logger.warning(f"í† í° ì œí•œìœ¼ë¡œ ë‹µë³€ì´ ì˜ë¦¼! (max_tokens={settings.OPENAI_MAX_TOKENS})")
                # í† í° ì œí•œìœ¼ë¡œ ì˜ë¦° ê²½ìš°ì—ë„ ë‹µë³€ì€ ë°˜í™˜ (ë¶€ë¶„ ë‹µë³€ì´ë¼ë„ ì˜ë¯¸ìˆìŒ)
                if answer and answer.strip():
                    answer += "\n\n(ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹œë©´ ë‹¤ì‹œ ë¬¼ì–´ë´ ì£¼ì„¸ìš”! ğŸ˜Š)"
                else:
                    logger.error("í† í° ì œí•œìœ¼ë¡œ ë¹ˆ ì‘ë‹µ ë°œìƒ")
                    return "ì§ˆë¬¸ì´ ì¡°ê¸ˆ ë³µì¡í–ˆë‚˜ë´ìš” ğŸ˜… ë” ê°„ë‹¨í•˜ê²Œ ë‹¤ì‹œ ë¬¼ì–´ë´ ì£¼ì‹œê² ì–´ìš”?"
            
            # ë¹ˆ ì‘ë‹µ ì²´í¬
            if not answer or answer.strip() == "":
                logger.warning(f"OpenAIê°€ ë¹ˆ ì‘ë‹µ ë°˜í™˜! finish_reason: {finish_reason}, í† í°: {response.usage.total_tokens}")
                return "ìŒ... ë­ë¼ê³  ë‹µí•´ì•¼ í• ì§€ ì˜ ëª¨ë¥´ê² ì–´ìš” ğŸ˜… ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ì–´ìš”?"
            
            # ë§ˆí¬ë‹¤ìš´ ë° íŠ¹ìˆ˜ ë¬¸ì ì œê±° (í›„ì²˜ë¦¬)
            answer = self._clean_response(answer)
            
            logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ (finish_reason: {finish_reason}, í† í°: {response.usage.total_tokens}, ê¸¸ì´: {len(answer)}ì)")
            
            return answer
            
        except Exception as e:
            logger.error(f"LLM ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}", exc_info=True)
            # í´ë°± ë‹µë³€
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _clean_response(self, text: str) -> str:
        """
        ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ë¦¬ - ë§ˆí¬ë‹¤ìš´, íŠ¹ìˆ˜ë¬¸ì ì œê±°
        """
        import re
        
        # ë§ˆí¬ë‹¤ìš´ bold ì œê±° (**text** -> text)
        text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
        
        # ë§ˆí¬ë‹¤ìš´ italic ì œê±° (*text* or _text_ -> text)
        text = re.sub(r'\*(.+?)\*', r'\1', text)
        text = re.sub(r'_(.+?)_', r'\1', text)
        
        # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±° (## text -> text)
        text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)
        
        # ë§ˆí¬ë‹¤ìš´ ë¦¬ìŠ¤íŠ¸ ì œê±° (- text or * text -> text)
        text = re.sub(r'^[\-\*]\s+', '', text, flags=re.MULTILINE)
        
        # ë°±ìŠ¬ë˜ì‹œ nì„ ì‹¤ì œ ì¤„ë°”ê¿ˆìœ¼ë¡œ (\\n -> \n)
        text = text.replace('\\n', '\n')
        
        # ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ (ëª¨ë°”ì¼ ìµœì í™”)
        text = re.sub(r'\n\s*\n', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
